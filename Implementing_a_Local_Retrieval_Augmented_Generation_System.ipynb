{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AxesAccess/Implementing-a-Local-Retrieval-Augmented-Generation-System/blob/main/Implementing_a_Local_Retrieval_Augmented_Generation_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_tOXtyaKqiN"
      },
      "source": [
        "# Implementing a Local Retrieval-Augmented Generation System"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrqHBxy6LIfR"
      },
      "source": [
        "This is the source code for the article [Implementing a Local Retrieval-Augmented Generation System](https://axesaccess.github.io/Blog/posts/20250321-rag/index.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4h-BLdmZjV11"
      },
      "outputs": [],
      "source": [
        "!pip install bitsandbytes faiss-cpu langchain langchain_community langchain_huggingface sentence-transformers --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bg3M-5_gzHpD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "import numpy as np\n",
        "import pickle\n",
        "from bs4 import BeautifulSoup, SoupStrainer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_core.documents import Document\n",
        "from langchain.vectorstores.faiss import FAISS\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AttAiboM1w-P"
      },
      "source": [
        "We will parse the Wikipedia category page to extract links to articles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Sm8TB2UyZxI"
      },
      "outputs": [],
      "source": [
        "from urllib.parse import urlparse\n",
        "\n",
        "\n",
        "def fetch_links(url):\n",
        "    links = []\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    domain = urlparse(url).netloc\n",
        "\n",
        "    for ul in soup.find_all(\"ul\"):\n",
        "        for li in ul.find_all(\"li\"):\n",
        "            link = li.find(\"a\")\n",
        "            if link and \"href\" in link.attrs:\n",
        "                href = link.attrs[\"href\"]\n",
        "                if \"/wiki\" in href[:5]:\n",
        "                    links.append(f\"https://{domain}{href}\")\n",
        "\n",
        "    return links"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2bBSvcD2Ri9"
      },
      "source": [
        "Set the `url` variable and get links."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37dqQRPWzGUQ"
      },
      "outputs": [],
      "source": [
        "url = \"https://en.wikipedia.org/wiki/Category:Machine_learning_algorithms\"\n",
        "links = fetch_links(url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MndYjgtd2qNl"
      },
      "source": [
        "Next, we will download articles as the docs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-e70HIJDE9mL"
      },
      "outputs": [],
      "source": [
        "os.environ[\"USER_AGENT\"] = (\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "    \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "    \"Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0\"\n",
        ")\n",
        "\n",
        "loader = WebBaseLoader(\n",
        "    links[20:],\n",
        "    bs_kwargs={\n",
        "        \"parse_only\": SoupStrainer(\"div\", {\"class\": \"mw-body-content\"}),\n",
        "    },\n",
        "    bs_get_text_kwargs={\"separator\": \" \", \"strip\": True},\n",
        ")\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYku7plb3g8Z"
      },
      "source": [
        "Sometimes session in Colab restarts, so we'll save results to pickle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YG6OtN_3bziQ"
      },
      "outputs": [],
      "source": [
        "with open(\"docs.pickle\", \"wb\") as f:\n",
        "    pickle.dump(docs, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwSY_pUQb_dU"
      },
      "outputs": [],
      "source": [
        "# Restore docs\n",
        "with open(\"docs.pickle\", \"rb\") as f:\n",
        "    docs = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuoNYa0I31mS"
      },
      "source": [
        "Here we break documents into shorter chunksâ€”overlapping parts that should be provided to the LLM as context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5tH8I5gFLVN"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "split_docs = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft8Y7IdR4Fe7"
      },
      "source": [
        "We need to perform quick search for relevant information, so let's transform texts to embeddings and load them into vector database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "md54BKCGAzLX"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "model_name = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
        "model_kwargs = {\"device\": \"cuda\"}\n",
        "encode_kwargs = {\"normalize_embeddings\": False}\n",
        "embedding = HuggingFaceEmbeddings(\n",
        "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgh_qwryI1GA"
      },
      "outputs": [],
      "source": [
        "vector_store = FAISS.from_documents(split_docs, embedding=embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V1r-KfJXHZ_"
      },
      "source": [
        "Here we define function for retrieving relevant documents from the database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTl_RqFv66ZJ"
      },
      "outputs": [],
      "source": [
        "encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "\n",
        "def retrieve(query, top_k=2):\n",
        "    query_vector = np.array(encoder.encode([query]))\n",
        "    documents = vector_store.search(query, \"similarity\")\n",
        "    return documents[:top_k]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5nq05sF820D"
      },
      "source": [
        "We will use local LLM. Let's authorize on HuggingFace which is mandatory for downloading certain models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgzc_mx2h97k"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "login(token=userdata.get(\"HF_TOKEN\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH_eNwXQEoqB"
      },
      "source": [
        "Download model and define tokenizer, and config."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sa3LrcIlaMFA"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
        "import torch\n",
        "\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-7B\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"cuda\",\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etcqh--F9R7n"
      },
      "source": [
        "Let's put context retrieval and generation pipeline into a function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKYe2R02U3xi"
      },
      "outputs": [],
      "source": [
        "gen_pipeline = pipeline(\n",
        "    \"text-generation\", model=model, tokenizer=tokenizer, return_full_text=False\n",
        ")\n",
        "\n",
        "\n",
        "def generate_response(query):\n",
        "    relevant_texts = retrieve(query)\n",
        "    context = \" \".join([t.model_dump()[\"page_content\"] for t in relevant_texts])\n",
        "    prompt = f\"\"\"Answer question using only information provided in the context.\n",
        "    If the context contains no relevant information, say \"I couldn't find the information\".\n",
        "    Context: '''{context}'''\n",
        "    Question: {query}\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "    response = gen_pipeline(prompt)\n",
        "    return response[0][\"generated_text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-YFSzO3tAW0"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnOUGEcb-J0Z"
      },
      "source": [
        "Here starts our Q&A session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lk9XQhUlZebC",
        "outputId": "134ba6e1-9f57-4c42-c714-7f082ce36dd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " The actor-critic algorithm (AC) is a family of reinforcement learning (RL) algorithms that combine policy-based RL algorithms such as policy gradient methods, and value-based RL algorithms such as value iteration, Q-learning, SARSA, and TD learning. An AC algorithm consists of two main components: an \"actor\" that determines which actions to take according to a policy function, and a \"critic\" that evaluates those actions according to a value function. Some AC algorithms are on-policy, some are off-policy. Some apply to either continuous or discrete action spaces. Some work in both cases.\n"
          ]
        }
      ],
      "source": [
        "query = \"What is the Actor-critic algorithm in reinforcement learning?\"\n",
        "\n",
        "answer = generate_response(query)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKYUu80gqAS2",
        "outputId": "c99e200a-f8c9-4eff-c53c-8859bc037d3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " The purpose of backpropagation in neural networks is to adjust the weights of the connections between neurons in order to minimize the error between the predicted output and the actual output. This is done by propagating the error backwards through the network, starting from the output layer and moving towards the input layer, hence the name \"backpropagation.\" The goal is to find the optimal set of weights that will allow the network to make accurate predictions on new, unseen data.\n"
          ]
        }
      ],
      "source": [
        "query = \"What is the purpose of backpropagation in neural networks?\"\n",
        "\n",
        "answer = generate_response(query)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "asrlUK3LTJUF",
        "outputId": "cbfb1397-80bc-4455-be01-5d22534e6b9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Curriculum learning in machine learning is a technique that involves gradually introducing more complex concepts or data to a model as it learns. This approach is inspired by the way humans learn, starting with simple concepts and building upon them. In the context provided, it is mentioned that this technique has its roots in the early study of neural networks, particularly in Jeffrey Elman's 1993 paper. Bengio et al. demonstrated successful application of curriculum learning in image classification tasks, such as identifying geometric shapes with increasingly complex forms, and language modeling tasks, such as training with a gradually expanding vocabulary. The authors conclude that curriculum strategies can be beneficial for machine learning models, especially when dealing with complex or large-scale problems.\n"
          ]
        }
      ],
      "source": [
        "query = \"Explain the concept of Curriculum learning in machine learning.\"\n",
        "\n",
        "answer = generate_response(query)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDt5X9FHu4Nm",
        "outputId": "dee3b5ff-aec8-44c9-d0b3-9faef652ef89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " The K-nearest neighbors (K-NN) algorithm classifies data by a plurality vote of its neighbors, with the object being assigned to the class most common among its K nearest neighbors (K is a positive integer, typically small). If K = 1, then the object is simply assigned to the class of that single nearest neighbor.\n"
          ]
        }
      ],
      "source": [
        "query = \"How does K-nearest neighbors (K-NN) algorithm classify data?\"\n",
        "\n",
        "answer = generate_response(query)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RNDUGXsvKUg",
        "outputId": "f2f2d5e5-cefa-490c-b807-3c62a9ee4aa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Federated Learning of Cohorts (FLoC) is a type of web tracking that groups people into \"cohorts\" based on their browsing history for the purpose of interest-based advertising. It was being developed as a part of Google's Privacy Sandbox initiative, which includes several other advertising-related technologies with bird-themed names. FLoC was being tested in Chrome 89 as a replacement for third-party cookies. Despite \"federated learning\" in the name, FLoC does not utilize any federated learning. FLoC improves data privacy by grouping people into cohorts based on their browsing history, rather than tracking individual users. This means that advertisers can still target users based on their interests, but without the need for individual user data.\n"
          ]
        }
      ],
      "source": [
        "query = \"What is Federated Learning of Cohorts and how does it improve data privacy?\"\n",
        "\n",
        "answer = generate_response(query)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhcuXj8H-j04"
      },
      "source": [
        "Looks good. Let's ask something that is not in the context. For instance, there was no articles on Transformer architecture among wiki articles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2Gnc5Y3ySG3",
        "outputId": "c0e491d3-f62b-44c7-895e-3c17fbc4236d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " The Transformer architecture improves upon traditional RNNs and LSTMs in NLP tasks by using self-attention mechanisms to capture long-range dependencies between words in a sentence. This allows the model to process entire sentences at once, rather than sequentially like RNNs and LSTMs. Additionally, the Transformer architecture uses a fixed-size attention mechanism, which makes it more efficient and scalable than RNNs and LSTMs.\n"
          ]
        }
      ],
      "source": [
        "query = (\n",
        "    \"How does the Transformer architecture improve upon traditional RNNs and LSTMs in NLP tasks?\"\n",
        ")\n",
        "\n",
        "answer = generate_response(query)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRlufLaC-_i0"
      },
      "source": [
        "That's interesting. To be sure that there's no information on this topic, let's check context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tI0T1IH-yrf9",
        "outputId": "a0a1a8cc-7a43-4a9a-d249-b6c28ce83252"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='a2ae5aee-3b78-4804-a983-25d08fb8f5d3', metadata={'source': 'https://en.wikipedia.org/wiki/Loss_functions_for_classification'}, page_content='Andrew Ng Fei-Fei Li Alex Krizhevsky Ilya Sutskever Demis Hassabis David Silver Ian Goodfellow Andrej Karpathy Architectures Neural Turing machine Differentiable neural computer Transformer Vision transformer (ViT) Recurrent neural network (RNN) Long short-term memory (LSTM) Gated recurrent unit (GRU) Echo state network Multilayer perceptron (MLP) Convolutional neural network (CNN) Residual neural network (RNN) Highway network Mamba Autoencoder Variational autoencoder (VAE) Generative adversarial network (GAN) Graph neural network (GNN) Portals Technology Category Artificial neural networks Machine learning List Companies Projects Retrieved from \" https://en.wikipedia.org/w/index.php?title=Loss_functions_for_classification&oldid=1261562183 \"'),\n",
              " Document(id='b267b523-9330-4b33-bc3a-b4e6edec109f', metadata={'source': 'https://en.wikipedia.org/wiki/Policy_gradient_method'}, page_content='neural computer Transformer Vision transformer (ViT) Recurrent neural network (RNN) Long short-term memory (LSTM) Gated recurrent unit (GRU) Echo state network Multilayer perceptron (MLP) Convolutional neural network (CNN) Residual neural network (RNN) Highway network Mamba Autoencoder Variational autoencoder (VAE) Generative adversarial network (GAN) Graph neural network (GNN) Portals Technology Category Artificial neural networks Machine learning List Companies Projects Retrieved from \" https://en.wikipedia.org/w/index.php?title=Policy_gradient_method&oldid=1280215280 \"')]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieve(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_bcXha1_SV_"
      },
      "source": [
        "It appears that the query retrieved random parts of pages mentioning transformers. However, as they contained no valuable information, the answer was fully generated by the LLM. Although the response was accurate, we may want to enhance the retrieval function by setting a threshold for relevancy to minimize the risk of hallucinations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eKJBf0xBhXQ"
      },
      "source": [
        "Let's ask a question from the completely different domain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yPjkdRgzGrM",
        "outputId": "606b3c85-663d-4799-d183-0a109d647edb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " I couldn't find the information.\n"
          ]
        }
      ],
      "source": [
        "query = \"How does the process of photosynthesis work in plants?\"\n",
        "\n",
        "answer = generate_response(query)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NL03GDywBxCl"
      },
      "source": [
        "This question left unanswered. What about another one?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gg9frZ2rzT9H",
        "outputId": "5a3279e6-67cc-4450-81ba-de32ed7d93c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Blockchain technology ensures security and decentralization through its decentralized nature and cryptographic algorithms. It operates on a distributed network of nodes, where each node maintains a copy of the entire blockchain. This means that no single entity has control over the entire system, making it resistant to tampering and censorship. Additionally, blockchain uses cryptographic algorithms to secure transactions and data, ensuring that only authorized parties can access and modify the information. This combination of decentralization and cryptographic security makes blockchain technology highly secure and decentralized.\n"
          ]
        }
      ],
      "source": [
        "query = \"How does blockchain technology ensure security and decentralization?\"\n",
        "\n",
        "answer = generate_response(query)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwCBDtV_B7oY"
      },
      "source": [
        "Unexpectedly, one of the documents contained information on this topic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DK_bNS2NzfkK",
        "outputId": "9946c931-bca2-4cf2-d035-58ee227e121c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='3d968d3b-1889-4435-b329-c9081400e8c4', metadata={'source': 'https://en.wikipedia.org/wiki/Augmented_Analytics'}, page_content='to democratising data: Data Parameterisation and Characterisation. Data Decentralisation using an OS of blockchain and DLT technologies, as well as an independently governed secure data exchange to enable trust. Consent Market-driven Data Monetisation. When it comes to connecting assets, there are two features that will accelerate the adoption and usage of data democratisation: decentralized identity management and business data object monetization of data ownership. It enables multiple individuals and organizations to identify, authenticate, and authorize participants and organizations, enabling them to access services, data or systems across multiple networks, organizations, environments, and use cases. It empowers users and enables a personalized, self-service digital onboarding system so that users can self-authenticate without relying on a central administration function to process their information. Simultaneously, decentralized identity management ensures the user is authorized'),\n",
              " Document(id='98578608-2a8d-4533-a655-b556202dda7d', metadata={'source': 'https://en.wikipedia.org/wiki/Augmented_Analytics'}, page_content='so that users can self-authenticate without relying on a central administration function to process their information. Simultaneously, decentralized identity management ensures the user is authorized to perform actions subject to the systemâ€™s policies based on their attributes (role, department, organization, etc.) and/ or physical location. [ 10 ] Use cases [ edit ] Agriculture  â€“ Farmers collect data on water use, soil temperature, moisture content and crop growth, augmented analytics can be used to make sense of this data and possibly identify insights that the user can then use to make business decisions. [ 11 ] Smart Cities  â€“ Many cities across the United States, known as Smart Cities collect large amounts of data on a daily basis. Augmented analytics can be used to simplify this data in order to increase effectiveness in city management (transportation, natural disasters, etc.). [ 11 ] Analytic Dashboards  â€“ Augmented analytics has the ability to take large data sets and create')]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieve(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEZJRo5jCTJn"
      },
      "source": [
        "One more question from another domain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nToVseC-0Zf0",
        "outputId": "8344a930-e9ab-46f1-cb3b-0008b25aef73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " I couldn't find the information.\n"
          ]
        }
      ],
      "source": [
        "query = \"What are the fundamental principles of classical mechanics?\"\n",
        "\n",
        "answer = generate_response(query)\n",
        "print(answer)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
