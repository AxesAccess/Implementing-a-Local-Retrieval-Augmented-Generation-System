{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AxesAccess/Implementing-a-Local-Retrieval-Augmented-Generation-System/blob/main/Implementing_a_Local_Retrieval_Augmented_Generation_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_tOXtyaKqiN"
      },
      "source": [
        "# Implementing a Local Retrieval-Augmented Generation System"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrqHBxy6LIfR"
      },
      "source": [
        "This is the source code for the article [Implementing a Local Retrieval-Augmented Generation System](https://axesaccess.github.io/Blog/posts/20250321-rag/index.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4h-BLdmZjV11"
      },
      "outputs": [],
      "source": [
        "!pip install bitsandbytes faiss-cpu faiss-gpu-cu12 langchain langchain_community langchain_huggingface sentence-transformers --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy\n",
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "id": "rM1VzDXZoG30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bg3M-5_gzHpD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import pickle\n",
        "from bs4 import BeautifulSoup, SoupStrainer\n",
        "from transformers import pipeline\n",
        "from langchain.vectorstores.faiss import FAISS\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AttAiboM1w-P"
      },
      "source": [
        "We will parse the Wikipedia category page to extract links to articles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3Sm8TB2UyZxI"
      },
      "outputs": [],
      "source": [
        "from urllib.parse import urlparse\n",
        "\n",
        "\n",
        "def fetch_links(url):\n",
        "    links = []\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    domain = urlparse(url).netloc\n",
        "\n",
        "    for ul in soup.find_all(\"ul\"):\n",
        "        for li in ul.find_all(\"li\"):\n",
        "            link = li.find(\"a\")\n",
        "            if link and \"href\" in link.attrs:\n",
        "                href = link.attrs[\"href\"]\n",
        "                if \"/wiki\" in href[:5]:\n",
        "                    links.append(f\"https://{domain}{href}\")\n",
        "\n",
        "    return links"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2bBSvcD2Ri9"
      },
      "source": [
        "Set the `url` variable and get links."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "37dqQRPWzGUQ"
      },
      "outputs": [],
      "source": [
        "url = \"https://en.wikipedia.org/wiki/Category:Machine_learning_algorithms\"\n",
        "links = fetch_links(url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MndYjgtd2qNl"
      },
      "source": [
        "Next, we will download articles as the docs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYku7plb3g8Z"
      },
      "source": [
        "Sometimes session in Colab restarts, so we'll save results to pickle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kwSY_pUQb_dU"
      },
      "outputs": [],
      "source": [
        "# Restore saved earlier docs\n",
        "try:\n",
        "    with open(\"docs.pickle\", \"rb\") as f:\n",
        "        docs = pickle.load(f)\n",
        "except FileNotFoundError:\n",
        "    os.environ[\"USER_AGENT\"] = (\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "        \"Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0\"\n",
        "    )\n",
        "    loader = WebBaseLoader(\n",
        "        # first 19 links are not relevant\n",
        "        links[20:],\n",
        "        bs_kwargs={\n",
        "            \"parse_only\": SoupStrainer(\"div\", {\"class\": \"mw-body-content\"}),\n",
        "        },\n",
        "        bs_get_text_kwargs={\"separator\": \" \", \"strip\": True},\n",
        "    )\n",
        "    docs = loader.load()\n",
        "    with open(\"docs.pickle\", \"wb\") as f:\n",
        "        pickle.dump(docs, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuoNYa0I31mS"
      },
      "source": [
        "Here we break documents into shorter chunks—overlapping parts that should be provided to the LLM as context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "W5tH8I5gFLVN"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "split_docs = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft8Y7IdR4Fe7"
      },
      "source": [
        "We need to perform quick search for relevant information, so let's transform texts to embeddings and load them into vector database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "md54BKCGAzLX"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "model_name = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
        "model_kwargs = {\"device\": \"cuda\"}\n",
        "encode_kwargs = {\"normalize_embeddings\": False}\n",
        "embedding = HuggingFaceEmbeddings(\n",
        "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zgh_qwryI1GA"
      },
      "outputs": [],
      "source": [
        "vector_store = FAISS.from_documents(split_docs, embedding=embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V1r-KfJXHZ_"
      },
      "source": [
        "Here we define function for retrieving relevant documents from the database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tTl_RqFv66ZJ"
      },
      "outputs": [],
      "source": [
        "def retrieve(query, top_k=2):\n",
        "    documents = vector_store.search(query, \"similarity\")\n",
        "    return documents[:top_k]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5nq05sF820D"
      },
      "source": [
        "We will use local LLM. Let's authorize on HuggingFace which is mandatory for downloading certain models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kgzc_mx2h97k"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "login(token=userdata.get(\"HF_TOKEN\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH_eNwXQEoqB"
      },
      "source": [
        "Download model and define tokenizer, and config."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sa3LrcIlaMFA"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
        "import torch\n",
        "\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-7B\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"cuda\",\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etcqh--F9R7n"
      },
      "source": [
        "Let's put context retrieval and generation pipeline into a function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "iKYe2R02U3xi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51fdaf98-1d16-4f01-cabc-53f3568529ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n"
          ]
        }
      ],
      "source": [
        "gen_pipeline = pipeline(\n",
        "    \"text-generation\", model=model, tokenizer=tokenizer, return_full_text=False\n",
        ")\n",
        "\n",
        "\n",
        "def generate_response(query):\n",
        "    relevant_texts = retrieve(query)\n",
        "    context = \" \".join([t.model_dump()[\"page_content\"] for t in relevant_texts])\n",
        "    prompt = f\"\"\"Answer question using only information provided in the context.\n",
        "    If the context contains no relevant information, say \"I couldn't find the information\".\n",
        "    Context: '''{context}'''\n",
        "    Question: {query}\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "    response = gen_pipeline(prompt)\n",
        "    return response[0][\"generated_text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "k-YFSzO3tAW0"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnOUGEcb-J0Z"
      },
      "source": [
        "Here starts our Q&A session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lk9XQhUlZebC",
        "outputId": "28c7bd1b-d39a-4c73-8297-6c9b46e11b01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The Actor-critic algorithm (AC) is a family of reinforcement learning (RL) algorithms that combine policy-based RL algorithms such as policy gradient methods, and value-based RL algorithms such as value iteration, Q-learning, SARSA, and TD learning. An AC algorithm consists of two main components: an \"actor\" that determines which actions to take according to a policy function, and a \"critic\" that evaluates those actions according to a value function. Some AC algorithms are on-policy, some are off-policy. Some apply to either continuous or discrete action spaces. Some work in both cases.\n"
          ]
        }
      ],
      "source": [
        "query = \"What is the Actor-critic algorithm in reinforcement learning?\"\n",
        "\n",
        "answer = generate_response(query)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKYUu80gqAS2",
        "outputId": "19b4ef28-2907-4fd5-a0ce-b3a997a44e97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The purpose of backpropagation in neural networks is to adjust the weights of the connections between neurons in order to minimize the error between the predicted output and the actual output. This is done by propagating the error backwards through the network, starting from the output layer and moving towards the input layer, hence the name \"backpropagation\". The goal is to iteratively update the weights so that the network can learn from its mistakes and improve its performance over time.\n"
          ]
        }
      ],
      "source": [
        "query = \"What is the purpose of backpropagation in neural networks?\"\n",
        "\n",
        "answer = generate_response(query)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "asrlUK3LTJUF",
        "outputId": "78bf636b-7644-478d-c0ca-adb8c9132add"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Curriculum learning in machine learning is a technique that involves gradually introducing more complex concepts or data to a model as it learns. This approach is inspired by the way humans learn, starting with simple concepts and building upon them. In the context provided, it is mentioned that Bengio et al. showed good results for problems in image classification and language modeling using curriculum learning strategies. They concluded that this technique can be beneficial for the model's performance on the test.\n"
          ]
        }
      ],
      "source": [
        "query = \"Explain the concept of Curriculum learning in machine learning.\"\n",
        "\n",
        "answer = generate_response(query)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDt5X9FHu4Nm",
        "outputId": "9fb6ceee-3950-44e5-bdff-b7007f9d8c82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The K-nearest neighbors (K-NN) algorithm classifies data by a plurality vote of its neighbors, with the object being assigned to the class most common among its K nearest neighbors (K is a positive integer, typically small). If K = 1, then the object is simply assigned to the class of that single nearest neighbor.\n"
          ]
        }
      ],
      "source": [
        "query = \"How does K-nearest neighbors (K-NN) algorithm classify data?\"\n",
        "\n",
        "answer = generate_response(query)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RNDUGXsvKUg",
        "outputId": "98fa30cc-2672-4bfd-fc2a-c29db433e10e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Federated Learning of Cohorts (FLoC) is a type of web tracking that groups people into \"cohorts\" based on their browsing history for the purpose of interest-based advertising. It was being developed as a part of Google's Privacy Sandbox initiative, which includes several other advertising-related technologies with bird-themed names. FLoC was being tested in Chrome 89 as a replacement for third-party cookies. Despite \"federated learning\" in the name, FLoC does not utilize any federated learning. FLoC improves data privacy by grouping people into cohorts based on their browsing history, rather than tracking individual users. This means that advertisers can still target users based on their interests, but without the need for individual user data.\n"
          ]
        }
      ],
      "source": [
        "query = \"What is Federated Learning of Cohorts and how does it improve data privacy?\"\n",
        "\n",
        "answer = generate_response(query)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhcuXj8H-j04"
      },
      "source": [
        "Looks good. Let's ask something that is not in the context. For instance, there was no articles on Transformer architecture among wiki articles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2Gnc5Y3ySG3",
        "outputId": "1938aab2-0bcb-48ea-da13-add1bd37c738"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The Transformer architecture improves upon traditional RNNs and LSTMs in NLP tasks by using self-attention mechanisms to capture long-range dependencies between words in a sentence. This allows the model to process entire sentences at once, rather than one word at a time, which can lead to better performance on tasks such as machine translation and language modeling. Additionally, the Transformer architecture is more efficient than traditional RNNs and LSTMs, as it does not require sequential processing of input data.\n"
          ]
        }
      ],
      "source": [
        "query = (\n",
        "    \"How does the Transformer architecture improve upon traditional RNNs and LSTMs in NLP tasks?\"\n",
        ")\n",
        "\n",
        "answer = generate_response(query)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRlufLaC-_i0"
      },
      "source": [
        "That's interesting. To be sure that there's no information on this topic, let's check context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tI0T1IH-yrf9",
        "outputId": "0bb4a59f-0405-4b1e-c2b9-a8e63d04ce8b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='8dfaf504-12a7-40ab-9681-caf72cb90e32', metadata={'source': 'https://en.wikipedia.org/wiki/Loss_functions_for_classification'}, page_content='Fei-Fei Li Alex Krizhevsky Ilya Sutskever Demis Hassabis David Silver Ian Goodfellow Andrej Karpathy Architectures Neural Turing machine Differentiable neural computer Transformer Vision transformer (ViT) Recurrent neural network (RNN) Long short-term memory (LSTM) Gated recurrent unit (GRU) Echo state network Multilayer perceptron (MLP) Convolutional neural network (CNN) Residual neural network (RNN) Highway network Mamba Autoencoder Variational autoencoder (VAE) Generative adversarial network (GAN) Graph neural network (GNN) Portals Technology Category Artificial neural networks Machine learning List Companies Projects Retrieved from \" https://en.wikipedia.org/w/index.php?title=Loss_functions_for_classification&oldid=1261562183 \"'),\n",
              " Document(id='c4a4a1c2-b912-41c8-8368-5e857c5da84a', metadata={'source': 'https://en.wikipedia.org/wiki/Policy_gradient_method'}, page_content='computer Transformer Vision transformer (ViT) Recurrent neural network (RNN) Long short-term memory (LSTM) Gated recurrent unit (GRU) Echo state network Multilayer perceptron (MLP) Convolutional neural network (CNN) Residual neural network (RNN) Highway network Mamba Autoencoder Variational autoencoder (VAE) Generative adversarial network (GAN) Graph neural network (GNN) Portals Technology Category Artificial neural networks Machine learning List Companies Projects Retrieved from \" https://en.wikipedia.org/w/index.php?title=Policy_gradient_method&oldid=1280215280 \"')]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "retrieve(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_bcXha1_SV_"
      },
      "source": [
        "It appears that the query retrieved random parts of pages mentioning transformers. However, as they contained no valuable information, the answer was fully generated by the LLM. Although the response was accurate, we may want to enhance the retrieval function by setting a threshold for relevancy to minimize the risk of hallucinations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eKJBf0xBhXQ"
      },
      "source": [
        "Let's ask a question from the completely different domain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yPjkdRgzGrM",
        "outputId": "9e5fbf7b-af1f-4ad2-f85d-ebaa72a33fb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " I couldn't find the information.\n"
          ]
        }
      ],
      "source": [
        "query = \"How does the process of photosynthesis work in plants?\"\n",
        "\n",
        "answer = generate_response(query)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NL03GDywBxCl"
      },
      "source": [
        "This question left unanswered. What about another one?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gg9frZ2rzT9H",
        "outputId": "96720a81-9864-4551-9b5a-b57c9e7625cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Blockchain technology ensures security and decentralization through its decentralized nature, where data is stored across a network of computers rather than a single central authority. This makes it more difficult for hackers to compromise the system, as they would need to hack multiple nodes simultaneously. Additionally, blockchain uses cryptographic algorithms to secure data and transactions, making it nearly impossible to alter or manipulate the data without detection.\n"
          ]
        }
      ],
      "source": [
        "query = \"How does blockchain technology ensure security and decentralization?\"\n",
        "\n",
        "answer = generate_response(query)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwCBDtV_B7oY"
      },
      "source": [
        "Unexpectedly, one of the documents contained information on this topic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DK_bNS2NzfkK",
        "outputId": "99c7bc5f-50c5-4d28-a7df-7d04b751de5a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='aca1234d-db32-450f-a279-8a9792b8535f', metadata={'source': 'https://en.wikipedia.org/wiki/Augmented_Analytics'}, page_content='to democratising data: Data Parameterisation and Characterisation. Data Decentralisation using an OS of blockchain and DLT technologies, as well as an independently governed secure data exchange to enable trust. Consent Market-driven Data Monetisation. When it comes to connecting assets, there are two features that will accelerate the adoption and usage of data democratisation: decentralized identity management and business data object monetization of data ownership. It enables multiple individuals and organizations to identify, authenticate, and authorize participants and organizations, enabling them to access services, data or systems across multiple networks, organizations, environments, and use cases. It empowers users and enables a personalized, self-service digital onboarding system so that users can self-authenticate without relying on a central administration function to process their information. Simultaneously, decentralized identity management ensures the user is authorized'),\n",
              " Document(id='d4165ca8-f7f6-44ca-8ac3-47136f74a46c', metadata={'source': 'https://en.wikipedia.org/wiki/Augmented_Analytics'}, page_content='so that users can self-authenticate without relying on a central administration function to process their information. Simultaneously, decentralized identity management ensures the user is authorized to perform actions subject to the system’s policies based on their attributes (role, department, organization, etc.) and/ or physical location. [ 10 ] Use cases [ edit ] Agriculture  – Farmers collect data on water use, soil temperature, moisture content and crop growth, augmented analytics can be used to make sense of this data and possibly identify insights that the user can then use to make business decisions. [ 11 ] Smart Cities  – Many cities across the United States, known as Smart Cities collect large amounts of data on a daily basis. Augmented analytics can be used to simplify this data in order to increase effectiveness in city management (transportation, natural disasters, etc.). [ 11 ] Analytic Dashboards  – Augmented analytics has the ability to take large data sets and create')]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "retrieve(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEZJRo5jCTJn"
      },
      "source": [
        "One more question from another domain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nToVseC-0Zf0",
        "outputId": "5f783202-a9de-40f0-ff2d-780cf4800b67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " I couldn't find the information.\n"
          ]
        }
      ],
      "source": [
        "query = \"What are the fundamental principles of classical mechanics?\"\n",
        "\n",
        "answer = generate_response(query)\n",
        "print(answer)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "pytorch-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}